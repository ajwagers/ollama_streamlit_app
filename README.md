# Ollama Streamlit App
I am building a streamlit interface for ollama and including RAG and Agent functionality.

v0 - This is a simple but functional webapp that allows you to chat with the LLM but has very little other features.

v0.1 - reworks some features and allows the user to select a model from their local Ollama instance.  Still skimpy on features.

v0.2 - Allows the user to set the client url and abstracted the API from the main script body.

v0.3 - Incorporated the LangChain libraries, added a Thinking... animation

v0.4 - Added the ability to save and reload chats.

v0.5 - Added a text input box for a RAG system as well as persistent memory across sessions.

v0.6 - Added support for multiple file types to be ingested as part of the RAG system.  Cleaned up some UI issues.

v0.7 - Made the sidebar collapsible by sections to clean up the appearance.  Also, added the ability to define a custom URL for the ChromaDB vector storage.
